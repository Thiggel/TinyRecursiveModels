#!/bin/bash -l

#SBATCH --job-name=maze-xlstm
#SBATCH --output=${WORK}/${REPO_NAME}/job_logs/maze-xlstm_%A.out
#SBATCH --error=${WORK}/${REPO_NAME}/job_logs/maze-xlstm_%A.err
#SBATCH --partition=a100
#SBATCH --gres=gpu:a100:2 -C a100_80
#SBATCH --time=21:00:00
#SBATCH --nodes=1

set -euo pipefail

unset SLURM_EXPORT_ENV
: "${WORK:?Set WORK to the base work directory}"
: "${SCRATCH:?Set SCRATCH to the base scratch directory}"

REPO_NAME=TinyRecursiveModels
REPO_DIR="${HOME}/${REPO_NAME}"
WORK_ROOT="${WORK}/${REPO_NAME}"
SCRATCH_ROOT="${SCRATCH}/${REPO_NAME}"
SIF_PATH="${WORK_ROOT}/containers/pytorch.sif"
PYTHON_USER_BASE="${WORK_ROOT}/python"
PIP_CACHE_DIR="${SCRATCH_ROOT}/pip-cache"
DATA_ROOT="${WORK_ROOT}/data"

BASE_CACHE_DIR="${SCRATCH_ROOT}"
HF_HOME="${BASE_CACHE_DIR}/hf"
HF_DATASETS_CACHE="${BASE_CACHE_DIR}/datasets"
HF_MODULES_CACHE="${BASE_CACHE_DIR}/modules"
TRANSFORMERS_CACHE="${BASE_CACHE_DIR}/transformers"
DEEPSPEED_CACHE_DIR="${BASE_CACHE_DIR}/deepspeed"
TRITON_CACHE_DIR="${BASE_CACHE_DIR}/triton"
WANDB_DIR="${BASE_CACHE_DIR}/wandb"
TORCH_HOME="${BASE_CACHE_DIR}/torch"
XDG_CACHE_HOME="${BASE_CACHE_DIR}/.cache"
PYTORCH_LIGHTNING_HOME="${BASE_CACHE_DIR}/lightning_logs"
HYDRA_BASE_DIR="${WORK_ROOT}/hydra"
CHECKPOINT_ROOT="${WORK_ROOT}/checkpoints"
JOB_LOG_DIR="${WORK_ROOT}/job_logs"
TMPDIR="${SLURM_TMPDIR:-${BASE_CACHE_DIR}/.tmp}"

COMMON_APPTAINER_ARGS_BASE=(
  --cleanenv
  --bind "${WORK}:${WORK}"
  --bind "${SCRATCH}:${SCRATCH}"
  --bind "${REPO_DIR}:${REPO_DIR}"
  --pwd "${REPO_DIR}"
  --env http_proxy=http://proxy:80
  --env https_proxy=http://proxy:80
  --env PYTHONUSERBASE="${PYTHON_USER_BASE}"
  --env PIP_CACHE_DIR="${PIP_CACHE_DIR}"
  --env PIP_DISABLE_PIP_VERSION_CHECK=1
  --env BASE_CACHE_DIR="${BASE_CACHE_DIR}"
  --env HF_HOME="${HF_HOME}"
  --env HF_DATASETS_CACHE="${HF_DATASETS_CACHE}"
  --env HF_MODULES_CACHE="${HF_MODULES_CACHE}"
  --env TRANSFORMERS_CACHE="${TRANSFORMERS_CACHE}"
  --env DEEPSPEED_CACHE_DIR="${DEEPSPEED_CACHE_DIR}"
  --env TRITON_CACHE_DIR="${TRITON_CACHE_DIR}"
  --env WANDB_DIR="${WANDB_DIR}"
  --env TORCH_HOME="${TORCH_HOME}"
  --env XDG_CACHE_HOME="${XDG_CACHE_HOME}"
  --env PYTORCH_LIGHTNING_HOME="${PYTORCH_LIGHTNING_HOME}"
  --env HYDRA_BASE_DIR="${HYDRA_BASE_DIR}"
  --env HYDRA_FULL_ERROR=1
  --env CHECKPOINT_ROOT="${CHECKPOINT_ROOT}"
  --env JOB_LOG_DIR="${JOB_LOG_DIR}"
  --env DATA_ROOT="${DATA_ROOT}"
  --env CUBLAS_WORKSPACE_CONFIG=:4096:8
  --env FLASH_ATTENTION_DETERMINISTIC=0
  --env TMPDIR="${TMPDIR}"
)

PYTHON_USER_SITE=$(apptainer exec "${COMMON_APPTAINER_ARGS_BASE[@]}" "${SIF_PATH}" \
  python -c 'import site, sys; sys.stdout.write(site.getusersitepackages())')

if [[ -z "${PYTHON_USER_SITE}" ]]; then
  echo "[maze-xlstm] Failed to resolve python user site directory" >&2
  exit 1
fi

COMMON_APPTAINER_ARGS=("${COMMON_APPTAINER_ARGS_BASE[@]}" --env "PYTHONPATH=${PYTHON_USER_SITE}")

RUN_NAME="pretrain_xlstm_maze30x30_a100x2"
PROJECT_NAME="Maze-ACT-torch"

DATA_DIR="${WORK_ROOT}/data/maze-30x30-hard-1k"
CHECKPOINT_DIR="${WORK_ROOT}/checkpoints/${RUN_NAME}"
HYDRA_DIR="${WORK_ROOT}/hydra/${RUN_NAME}/${SLURM_JOB_ID:-manual}"

if [[ ! -f "${SIF_PATH}" ]]; then
  echo "[maze-xlstm] Missing Apptainer image at ${SIF_PATH}. Run jobs/setup.sh first." >&2
  exit 1
fi

if [[ ! -d "${DATA_DIR}" ]]; then
  echo "[maze-xlstm] Dataset not found at ${DATA_DIR}. Run jobs/setup.sh to prepare datasets." >&2
  exit 1
fi

mkdir -p "${JOB_LOG_DIR}" "${CHECKPOINT_DIR}" "${HYDRA_DIR}" \
         "${PIP_CACHE_DIR}" "${HF_HOME}" "${HF_DATASETS_CACHE}" \
         "${HF_MODULES_CACHE}" "${TRANSFORMERS_CACHE}" \
         "${DEEPSPEED_CACHE_DIR}" "${TRITON_CACHE_DIR}" \
         "${WANDB_DIR}" "${TORCH_HOME}" "${XDG_CACHE_HOME}" \
         "${PYTORCH_LIGHTNING_HOME}"

if [[ -z "${SLURM_TMPDIR:-}" ]]; then
  mkdir -p "${TMPDIR}"
fi

cd "${REPO_DIR}"

apptainer exec --nv "${COMMON_APPTAINER_ARGS[@]}" \
  "${SIF_PATH}" bash -lc "
    set -euo pipefail
    python3.10 -m pip install --user -r requirements.txt
    torchrun --nproc-per-node=2 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \
      arch=trm \
      data_paths=[\"${DATA_DIR}\"] \
      evaluators=\"[]\" \
      epochs=50000 eval_interval=5000 \
      lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0 \
      arch.L_layers=2 \
      arch.H_cycles=3 arch.L_cycles=4 \
      arch.depth_recurrence=xlstm \
      arch.depth_cell_layers=2 \
      +run_name=${RUN_NAME} \
      project_name=${PROJECT_NAME} \
      checkpoint_path=\"${CHECKPOINT_DIR}\" \
      hydra.run.dir=\"${HYDRA_DIR}\" \
      ema=True
  "
